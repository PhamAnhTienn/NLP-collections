{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9348575,"sourceType":"datasetVersion","datasetId":5666450}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#there is no activation function applied to the output layer. CBOW is essentially a simple neural network with a linear layer \n#(also known as the projection layer) that maps the input (context words) to the output (target word).","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport spacy","metadata":{"_uuid":"0a440b07-bdef-4d70-b1b7-152dd5d62388","_cell_guid":"17c8a945-b302-4eac-9a9c-1c82f21ca8af","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-09T04:32:54.808431Z","iopub.execute_input":"2024-09-09T04:32:54.808856Z","iopub.status.idle":"2024-09-09T04:32:54.814351Z","shell.execute_reply.started":"2024-09-09T04:32:54.808818Z","shell.execute_reply":"2024-09-09T04:32:54.813011Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CBOW(nn.Module):\n    def __init__(self, embedding_size = 128, vocab_size = -1):\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.linear = nn.Linear(embedding_size, vocab_size)\n    \n    def forward(self, inputs):\n        embeddings = self.embeddings(inputs).mean(1).squeeze(1) # batch_size x embedding_size\n        return self.linear(embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T04:34:26.898600Z","iopub.execute_input":"2024-09-09T04:34:26.899099Z","iopub.status.idle":"2024-09-09T04:34:26.907411Z","shell.execute_reply.started":"2024-09-09T04:34:26.899051Z","shell.execute_reply":"2024-09-09T04:34:26.905981Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def data_genarator():\n    with open(\"/kaggle/input/raw-text/raw.txt\", \"r\", encoding = \"utf-8\") as f:\n        raw_text = f.read()\n        \n    nlp = spacy.load(\"en_core_web_sm\")\n    tokenized_text = [token.text for token in nlp(raw_text)]\n    vocab = set(tokenized_text)\n    \n    word_to_idx = { word : i for i, word in enumerate(vocab)}\n    idx_to_word = { i : word for i, word in enumerate(vocab)}\n    \n    data = []\n    for i in range(2, len(tokenized_text)-2):\n        #take 2 words before and 2 words after\n        context = [\n            tokenized_text[i-2],\n            tokenized_text[i-1],\n            tokenized_text[i+1],\n            tokenized_text[i+2],\n        ]\n        #take target word\n        target = tokenized_text[i]\n        \n        context_ids = [word_to_idx[w] for w in context]\n        target_id = word_to_idx[target]\n        data.append((context_ids, target_id))\n    \n    return data, word_to_idx, idx_to_word","metadata":{"execution":{"iopub.status.busy":"2024-09-09T04:32:58.943089Z","iopub.execute_input":"2024-09-09T04:32:58.944111Z","iopub.status.idle":"2024-09-09T04:32:58.952896Z","shell.execute_reply.started":"2024-09-09T04:32:58.944063Z","shell.execute_reply":"2024-09-09T04:32:58.951625Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data, word_to_idx, idx_to_word = data_genarator()\nloss_func = nn.CrossEntropyLoss()\nmodel = CBOW(vocab_size = len(word_to_idx))\noptimizer = optim.Adam(model.parameters(), lr = 1e-4)\n\n#convert to tensors\ncontexts = torch.tensor([ex[0] for ex in data])\ntargets = torch.tensor([ex[1] for ex in data])\n\n#create dataset from tensors and datataloader\ndataset = torch.utils.data.TensorDataset(contexts, targets)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle = True)\n\n#train\nfor epoch in range(25):\n    for context, target in dataloader:\n        output = model(context)\n        loss = loss_func(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch : {epoch}, Loss : {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T04:36:20.686862Z","iopub.execute_input":"2024-09-09T04:36:20.687312Z","iopub.status.idle":"2024-09-09T04:36:27.866145Z","shell.execute_reply.started":"2024-09-09T04:36:20.687269Z","shell.execute_reply":"2024-09-09T04:36:27.864875Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch : 0, Loss : 6.713914394378662\nEpoch : 1, Loss : 6.592629432678223\nEpoch : 2, Loss : 6.564905643463135\nEpoch : 3, Loss : 6.512890815734863\nEpoch : 4, Loss : 6.348079204559326\nEpoch : 5, Loss : 6.458359718322754\nEpoch : 6, Loss : 6.294544219970703\nEpoch : 7, Loss : 6.266358375549316\nEpoch : 8, Loss : 6.227489471435547\nEpoch : 9, Loss : 5.98853063583374\nEpoch : 10, Loss : 6.073175430297852\nEpoch : 11, Loss : 6.195258140563965\nEpoch : 12, Loss : 6.098715782165527\nEpoch : 13, Loss : 6.042919635772705\nEpoch : 14, Loss : 5.451259613037109\nEpoch : 15, Loss : 5.844895362854004\nEpoch : 16, Loss : 5.879215240478516\nEpoch : 17, Loss : 5.399789810180664\nEpoch : 18, Loss : 5.50230598449707\nEpoch : 19, Loss : 5.100342750549316\nEpoch : 20, Loss : 5.287597179412842\nEpoch : 21, Loss : 5.580905437469482\nEpoch : 22, Loss : 5.354588031768799\nEpoch : 23, Loss : 5.023834228515625\nEpoch : 24, Loss : 5.487366199493408\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}